{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4b7142",
   "metadata": {},
   "source": [
    "# Introduction to Principal Component Analysis (PCA) for Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446fdc4",
   "metadata": {},
   "source": [
    "1. Basics of PCA.\n",
    "2. Calculate PCA from scratch with NumPy.\n",
    "3. Using PCA from sklearn.decomposition.\n",
    "4. Some applications of PCA.\n",
    "5. What are the assumptions and limitations of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcac844",
   "metadata": {},
   "source": [
    "## 1. Basics of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45700249",
   "metadata": {},
   "source": [
    "* Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data.\n",
    "* It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef54e63",
   "metadata": {},
   "source": [
    "* The first step is to **calculate the mean values of each column**.\n",
    "* Next, we need to **center the values in each column by subtracting the mean** column value.\n",
    "* The next step is to **calculate the covariance matrix of the centered matrix**.\n",
    "    * **Correlation is a normalized measure** of the amount and direction (positive or negative) that two columns change together.\n",
    "    * **Covariance is a generalized and unnormalized** version of correlation across multiple columns.\n",
    "    * A **covariance matrix** is a calculation of covariance of a given matrix with covariance scores for every column with every other column, including itself.\n",
    "* Finally, we calculate the **eigendecomposition of the covariance matrix**. This results in a list of eigenvalues and a list of eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411b1bb",
   "metadata": {},
   "source": [
    "* The eigenvectors can be sorted by the eigenvalues in descending order to provide a ranking of the components or axes of the new subspace for A.\n",
    "* **If all eigenvalues have a similar value**, then we know that the existing representation may already be reasonably compressed or dense and that the projection may offer little.\n",
    "* If there are **eigenvalues close to zero**, they represent components or axes of B that may be discarded.\n",
    "* A total of **m or less components must be selected** to comprise the chosen subspace. Ideally, we would **select k eigenvectors, called principal components**, that have the **k largest eigenvalues**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1f928",
   "metadata": {},
   "source": [
    "Other matrix decomposition methods can be used such as **Singular-Value Decomposition**, or SVD. As such, generally the values are referred to as singular values and the vectors of the subspace are referred to as principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf3fd7",
   "metadata": {},
   "source": [
    "Once chosen, data can be projected into the subspace via matrix multiplication:\n",
    "\n",
    "P = B^T . A\n",
    "\n",
    "* A is the original data that we wish to project.\n",
    "* B^T is the transpose of the chosen principal components (vectors).\n",
    "* P is the projection of A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083a151",
   "metadata": {},
   "source": [
    "This is called the **covariance method for calculating the PCA**, although there are alternative ways to to calculate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa140b8",
   "metadata": {},
   "source": [
    "## 2. Calculate PCA from scratch with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5330e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b6e7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 4.]\n",
      " [4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean of each column\n",
    "M = mean(A.T, axis=1)\n",
    "\n",
    "# center columns by subtracting column means\n",
    "C = A - M\n",
    "\n",
    "# calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5009971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "[8. 0.]\n"
     ]
    }
   ],
   "source": [
    "# eigendecomposition of covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(vectors)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029c1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "print(P.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4a41f",
   "metadata": {},
   "source": [
    "## 3. Using PCA from sklearn.decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca8484fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "[8.00000000e+00 2.25080839e-33]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# create a PCA instance and fit the data\n",
    "pca = PCA(2)\n",
    "pca.fit(A)\n",
    "# access vectros and values\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384ab377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.82842712e+00  2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82842712e+00 -2.22044605e-16]]\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "B = pca.transform(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dd2c4",
   "metadata": {},
   "source": [
    "http://scikit-learn.sourceforge.net/dev/modules/decomposition.html#principal-component-analysis-pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ede1e",
   "metadata": {},
   "source": [
    "## 4. Some applications of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef84c9f",
   "metadata": {},
   "source": [
    "* PCA is predominantly used as a **dimensionality reduction** technique in domains like **facial recognition, computer vision and image compression**. \n",
    "* It is also used for **finding patterns in data of high dimension** in the field of **finance, data mining, bioinformatics, psychology**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42f1fe",
   "metadata": {},
   "source": [
    "https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b781c4a",
   "metadata": {},
   "source": [
    "1. Visualize multidimensional data.\n",
    "2. Compress information (images, signal processing).\n",
    "3. Simplify complex business decisions (Finance: risk management of interest rate derivative portfolios).\n",
    "4. Clarify convoluted scientific processes (related to neural ensembles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62931e",
   "metadata": {},
   "source": [
    "https://glowingpython.blogspot.com/2011/07/principal-component-analysis-with-numpy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df28b7",
   "metadata": {},
   "source": [
    "https://glowingpython.blogspot.com/2011/07/pca-and-image-compression-with-numpy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb309bfc",
   "metadata": {},
   "source": [
    "## 5. What are the assumptions and limitations of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96b692",
   "metadata": {},
   "source": [
    "1. PCA assumes a correlation between features.\n",
    "2. PCA is sensitive to the scale of the features.\n",
    "3. PCA is not robust against outliers.\n",
    "4. PCA assumes a linear relationship between features.\n",
    "5. Technical implementations often assume no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a57047",
   "metadata": {},
   "source": [
    "https://www.keboola.com/blog/pca-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa399f",
   "metadata": {},
   "source": [
    "https://www.simplilearn.com/tutorials/machine-learning-tutorial/principal-component-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65af95",
   "metadata": {},
   "source": [
    "https://www.mygreatlearning.com/blog/understanding-principal-component-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc723126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
